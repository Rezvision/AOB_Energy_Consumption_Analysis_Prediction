{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "import mifs\n",
    "from boruta import BorutaPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime.lime_tabular\n",
    "# Create a LIME explainer for regression\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns.tolist(),\n",
    "    mode='regression'\n",
    ")\n",
    "\n",
    "# Explain an instance from the test set for regression\n",
    "idx = 5  # Index of the instance to explain\n",
    "exp = explainer.explain_instance(X_test.iloc[idx], rf_model.predict, num_features=10)\n",
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer\n",
    "\n",
    "def periodic_spline_transformer(period, n_splines=None, degree=3):\n",
    "    if n_splines is None:\n",
    "        n_splines = period\n",
    "    n_knots = n_splines + 1  # periodic and include_bias is True\n",
    "    return SplineTransformer(\n",
    "        degree=degree,\n",
    "        n_knots=n_knots,\n",
    "        knots=np.linspace(0, period, n_knots).reshape(n_knots, 1),\n",
    "        extrapolation=\"periodic\",\n",
    "        include_bias=True,\n",
    "    )\n",
    "\n",
    "# Apply Spline Transformation\n",
    "def apply_spline_transformer(df, column, period):\n",
    "    spline_transformer = periodic_spline_transformer(period)\n",
    "    transformed_values = spline_transformer.fit_transform(df[[column]])\n",
    "    \n",
    "    # Create new column names for the transformed features\n",
    "    transformed_cols = [f\"{column}_spline_{i}\" for i in range(transformed_values.shape[1])]\n",
    "    df[transformed_cols] = transformed_values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply spline transformation for the temporal features\n",
    "df = apply_spline_transformer(df, 'month', 12)\n",
    "df = apply_spline_transformer(df, 'hour', 24)\n",
    "df = apply_spline_transformer(df, 'day of week', 7)\n",
    "df = apply_spline_transformer(df, 'day of month', 31)\n",
    "\n",
    "# Additional feature engineering\n",
    "df['working_hours'] = df['hour'].apply(lambda x: 8 <= x <= 17)\n",
    "df['bank holiday'] = df['bank holiday'].astype(int)\n",
    "df['weekend'] = df['weekend'].astype(int)\n",
    "\n",
    "# Drop the original columns if no longer needed\n",
    "df.drop(columns=['month', 'hour', 'day of week', 'day of month'], inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Lasso\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "lasso_features = (lasso.coef_ != 0)\n",
    "\n",
    "# Feature Selection with Random Forest and Boruta\n",
    "rf = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "boruta_selector = BorutaPy(rf, n_estimators='auto', random_state=42)\n",
    "boruta_selector.fit(X_train_scaled, y_train)\n",
    "boruta_features = boruta_selector.support_\n",
    "\n",
    "# Feature Selection with SHAP and RandomForest\n",
    "rf_shap = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_shap.fit(X_test_scaled, y_test)\n",
    "explainer = shap.TreeExplainer(rf_shap)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "shap_sum = np.abs(shap_values).mean(axis=0)\n",
    "shap_features = shap_sum > np.percentile(shap_sum, 75)  # Top 25% SHAP values\n",
    "print(shap_features)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Fit an SVR model\n",
    "svr = SVR(kernel='linear')\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract feature importance based on the coefficients\n",
    "svr_features = np.abs(svr.coef_[0]) > np.percentile(np.abs(svr.coef_[0]), 75)  # Top 25% features\n",
    "print(\"Features selected by SVR (Top 25%):\", svr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_feature_importances(X_columns, lasso_features, boruta_features, shap_features, svr_features, weights=None):\n",
    "    if weights is None:\n",
    "        weights = {'lasso': 1, 'boruta': 1, 'shap': 1, 'svr': 1}\n",
    "    \n",
    "    # Initialize scores array\n",
    "    feature_scores = np.zeros(len(X_columns))\n",
    "\n",
    "    # Assign scores based on feature selection outcomes\n",
    "    feature_scores += weights['lasso'] * lasso_features\n",
    "    feature_scores += weights['boruta'] * boruta_features\n",
    "    feature_scores += weights['shap'] * shap_features\n",
    "    feature_scores += weights['svr'] * svr_features\n",
    "\n",
    "    # Normalize scores\n",
    "    max_score = np.max(feature_scores)\n",
    "    if max_score > 0:\n",
    "        feature_scores /= max_score\n",
    "    \n",
    "    # Decide on a threshold to select features\n",
    "    selected_features = feature_scores > 0.5  # Select features with a score above 0.5\n",
    "   \n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# Combine feature importances\n",
    "combined_features = combine_feature_importances(\n",
    "    X_columns=X_train.columns, \n",
    "    lasso_features=lasso_features,\n",
    "    boruta_features=boruta_features,\n",
    "    shap_features=shap_features,\n",
    "    svr_features=svr_features\n",
    "    #weights={'lasso': 1, 'boruta': 1.5, 'shap': 1.5, 'svr': 1}\n",
    ")\n",
    "\n",
    "print(\"Combined Feature Selection:\")\n",
    "print(X_train.columns[combined_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cheating metrics \n",
    "def cv_rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Coefficient of Variation of Root Mean Square Error (CV(RMSE))\n",
    "    :param y_true: array-like of true values\n",
    "    :param y_pred: array-like of predicted values\n",
    "    :return: CV(RMSE) value\n",
    "    \"\"\"\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    mean_y_true = np.mean(y_true)\n",
    "    cv_rmse_value = rmse / mean_y_true\n",
    "    return cv_rmse_value\n",
    "\n",
    "def md_mape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Median Absolute Percentage Error (MD(MAPE))\n",
    "    :param y_true: array-like of true values\n",
    "    :param y_pred: array-like of predicted values\n",
    "    :return: MD(MAPE) value\n",
    "    \"\"\"\n",
    "    ape = np.abs((y_true - y_pred) / y_true) * 100\n",
    "    md_mape_value = np.median(ape)\n",
    "    return md_mape_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter training and testing sets with combined features\n",
    "X_train_selected = X_train.loc[:, combined_features]\n",
    "X_test_selected = X_test.loc[:, combined_features]\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Model Training\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"SVR\": SVR()\n",
    "}\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Training Errors\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_smape = smape(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    train_cv_rmse = cv_rmse(y_train, y_train_pred)\n",
    "    train_mdmape = md_mape(y_train, y_train_pred)\n",
    "\n",
    "    # Testing Errors\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_smape = smape(y_test, y_test_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_cv_rmse = cv_rmse(y_test, y_test_pred)\n",
    "    test_mdmape = md_mape(y_test, y_test_pred)\n",
    "\n",
    "    # Normalized RMSE\n",
    "    y_mean = np.mean(y_test)\n",
    "    test_nrmse = test_rmse / y_mean\n",
    "\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"  Train SMAPE: {train_smape:.2f}%, Test SMAPE: {test_smape:.2f}%\")\n",
    "    print(f\"  Train R^2: {train_r2:.4f}, Test R^2: {test_r2:.4f}\")\n",
    "    print(f\"  Test NRMSE: {test_nrmse:.4f}\")\n",
    "    print(f\"  Train CV RMSE: {train_cv_rmse:.4f}, Test CV RMSE: {test_cv_rmse:.4f}\")\n",
    "    print(f\"  Train Median Mape: {train_mdmape:.4f}, Test Median Mape: {test_mdmape:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(n_jobs=-1, random_state=42),\n",
    "    \"SVR\": SVR(kernel=\"linear\"),\n",
    "    \"Lasso\": Lasso(alpha=0.01),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Dictionary to store the top features for each model\n",
    "top_features = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Initialize RFE with the model and step=1 to remove one feature at a time\n",
    "    rfe_selector = RFE(estimator=model, n_features_to_select=1, step=1)\n",
    "\n",
    "    # Fit RFE\n",
    "    rfe_selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the ranking of the features\n",
    "    rfe_ranking = rfe_selector.ranking_\n",
    "\n",
    "    # Map the feature names to their RFE ranking\n",
    "    feature_names = X_train.columns\n",
    "    rfe_ranking_named = {feature_names[i]: rfe_ranking[i] for i in range(len(feature_names))}\n",
    "\n",
    "    # Sort features by ranking and get the top 10\n",
    "    sorted_features = sorted(rfe_ranking_named.items(), key=lambda x: x[1])\n",
    "    top_features[model_name] = sorted_features[:10]\n",
    "\n",
    "    # Print the top features for the model\n",
    "    print(f\"Top 10 Features for {model_name}:\")\n",
    "    for feature, rank in top_features[model_name]:\n",
    "        print(f\"{feature}: {rank}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Create a DataFrame to compare the top features for each model\n",
    "comparison_df = pd.DataFrame({model: [feature for feature, rank in top_features[model]] for model in top_features})\n",
    "print(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
